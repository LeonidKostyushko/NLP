{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "\n",
    "#### 1.How did you decide to handle this amount of data\n",
    "\n",
    "Uploading csvs in pandas DataFrame. Text extraction using BeautifulSoap.\n",
    "\n",
    "\n",
    "#### 2.How did you decide to do feature engineering \n",
    "\n",
    "I've decided to make vocabulary by words from keyword2tumor_type.csv as they are already related to targets. And then I've created a tf/idf embedding. \n",
    "\n",
    "#### 3.How did you decide which models to try (if you decide to train any models)\n",
    "\n",
    "As we have a classification problem. Firstly, I've decided to use simple logistic regression to fastly test the results. After I made sure it works I've decided to use 2 state of the art models: random forest and gradient boosting.\n",
    "\n",
    "#### 4.How did you perform validation of your model\n",
    "\n",
    "Using KFolf from sklearn. And after extraction the best training subset I've tuned the parameters of the models also on a validation.\n",
    "\n",
    "#### 5.What metrics did you measure\n",
    "Weighted recision, recall, F1 as we have imbalanced classes. If there is more data I'd also use roc-auc.\n",
    "\n",
    "#### 6.How do you expect your model to perform on test data (in terms of your metrics)\n",
    "\n",
    "On validation.\n",
    "\n",
    "\n",
    "#### 7.How fast will your algorithm performs and how could you improve its performance if you would have more time\n",
    "\n",
    "From the report it's few seconds now. As more data we have as more time it will take. To improve it I'd prefer to use parallel techniques, batch iteration and fitting on GPUs.\n",
    "\n",
    "#### 8.How do you think you would be able to improve your algorithm if you would have more data\n",
    "\n",
    "Extracting more relevant words to use in vocabulary. With bigger dimension it would be computationally harder to use tf/idf so it may be replaced by other embeddings such as word2vec. Also as you may see below I've used both training and testing data to create tf/idf embedding. With more data it will not be necessary to refit the model each time beacause the idf coeficcient will converge to the probability (as it uses the amounts of documents) and it we will only need to compute tfs for new docs.\n",
    "\n",
    "#### 9.What potential issues do you see with your algorithm\n",
    "A small amount of data. It's hard to find patterns for algorithm. It should also be better features extracting: try to find new relevant words that are not in keyword2tumor_type.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Desktop/train.csv')\n",
    "keywords_df = pd.read_csv('Desktop/keyword2tumor_type.csv')\n",
    "test_df = pd.read_csv('Desktop/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://elbe-elster-klinikum.de/fachbereiche/ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://klinikum-bayreuth.de/einrichtungen/zent...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://klinikum-braunschweig.de/info.php/?id_o...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://klinikum-braunschweig.de/info.php/?id_o...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://klinikum-braunschweig.de/zuweiser/tumor...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  doc_id  label\n",
       "0  http://elbe-elster-klinikum.de/fachbereiche/ch...       1      1\n",
       "1  http://klinikum-bayreuth.de/einrichtungen/zent...       3      3\n",
       "2  http://klinikum-braunschweig.de/info.php/?id_o...       4      1\n",
       "3  http://klinikum-braunschweig.de/info.php/?id_o...       5      1\n",
       "4  http://klinikum-braunschweig.de/zuweiser/tumor...       6      3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df['url'].shape)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://chirurgie-goettingen.de/medizinische-ve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://evkb.de/kliniken-zentren/chirurgie/allg...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://krebszentrum.kreiskliniken-reutlingen.d...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://marienhospital-buer.de/mhb-av-chirurgie...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://marienhospital-buer.de/mhb-av-chirurgie...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  doc_id\n",
       "0  http://chirurgie-goettingen.de/medizinische-ve...       0\n",
       "1  http://evkb.de/kliniken-zentren/chirurgie/allg...       2\n",
       "2  http://krebszentrum.kreiskliniken-reutlingen.d...       7\n",
       "3  http://marienhospital-buer.de/mhb-av-chirurgie...      15\n",
       "4  http://marienhospital-buer.de/mhb-av-chirurgie...      16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_df['url'].shape)\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>tumor_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>senologische</td>\n",
       "      <td>Brust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brustzentrum</td>\n",
       "      <td>Brust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>breast</td>\n",
       "      <td>Brust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thorax</td>\n",
       "      <td>Brust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thorakale</td>\n",
       "      <td>Brust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        keyword tumor_type\n",
       "0  senologische      Brust\n",
       "1  brustzentrum      Brust\n",
       "2        breast      Brust\n",
       "3        thorax      Brust\n",
       "4     thorakale      Brust"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_label_dict = {1:0, 2:0, 3:0}\n",
    "for label in train_df['label']:\n",
    "    data_per_label_dict[label]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 32, 2: 59, 3: 9}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(data_per_label_dict.keys(), data_per_label_dict.values(), align='center', alpha=1, color = 'blue')\n",
    "plt.title('Objects per label')\n",
    "print(data_per_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => classes are imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary lenght:  126\n"
     ]
    }
   ],
   "source": [
    "print(\"Dictionary lenght: \", len(keywords_df['keyword']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = []\n",
    "data_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('german'))   \n",
    "stop_words.add('javascript')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "vocab = set(keywords_df['keyword'])\n",
    "diction = []\n",
    "for phrase in vocab:\n",
    "    for word in phrase.split(' '):\n",
    "        word.replace('-','')\n",
    "        if len(word) > 2:\n",
    "            diction.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "def preprocess(ind, url, is_train = True):\n",
    "    if ind == 85:\n",
    "        return \n",
    "    r  = requests.get(url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "    \n",
    "    tags = soup.find_all('p')\n",
    "    text = ''.join(tag.text + ' ' for tag in tags)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub('\\W+',' ', text)\n",
    "    text = re.sub(r'_',' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    tokens_no_stpwrd = [i for i in words if not i in stop_words and len(i) > 2]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens_no_stpwrd]\n",
    "    \n",
    "    result_tokens = [token for token in tokens if token in diction]\n",
    "    \n",
    "    result_text = ''.join(token + ' ' for token in result_tokens)\n",
    "    \n",
    "    if len(result_text) != 0:\n",
    "        data_X.append(result_text)\n",
    "        if is_train:\n",
    "            data_y.append(train_df['label'][ind])\n",
    "        else:\n",
    "            test_y_id.append(test_df['doc_id'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,url in enumerate(train_df['url']):\n",
    "    preprocess(ind, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data_X\n",
    "train_y = data_y\n",
    "data_X = []\n",
    "test_y_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,url in enumerate(test_df['url']):\n",
    "    preprocess(ind, url, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_vocab = train_X + test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data_for_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.toarray()\n",
    "train_X = X[:len(train_X)]\n",
    "test_X = X[len(train_X):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log_reg = LogisticRegression()\n",
    "kf = KFold(n_splits=5)\n",
    "best_f1 = -1\n",
    "f1_l = []\n",
    "pr_l = []\n",
    "rec_l = []\n",
    "\n",
    "for train_ind, test_ind in kf.split(train_X,train_y):\n",
    "    clf_log_reg.fit(train_X[train_ind], train_y[train_ind])\n",
    "    predictions = clf_log_reg.predict(train_X[test_ind])\n",
    "    \n",
    "    f1 = f1_score(train_y[test_ind],predictions, average='weighted')\n",
    "    f1_l.append(f1)\n",
    "    pr = precision_score(train_y[test_ind],predictions, average='weighted')\n",
    "    pr_l.append(pr)\n",
    "    rec = recall_score(train_y[test_ind],predictions, average='weighted')\n",
    "    rec_l.append(rec)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = max(best_f1, f1)\n",
    "        best_X_train = train_X[train_ind]\n",
    "        best_y_train = train_y[train_ind]\n",
    "        best_X_test = train_X[test_ind]\n",
    "        best_y_test = train_y[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 vallidation:         [0.4688644688644689, 0.4688644688644689, 0.6688963210702342, 0.4688644688644689, 0.5333333333333333]\n",
      "Precision vallidation:  [0.37869822485207105, 0.37869822485207105, 0.591715976331361, 0.37869822485207105, 0.4444444444444444]\n",
      "Recall vallidation:     [0.6153846153846154, 0.6153846153846154, 0.7692307692307693, 0.6153846153846154, 0.6666666666666666]\n",
      "Best F1:  0.6688963210702342\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 vallidation:        \", f1_l)\n",
    "print(\"Precision vallidation: \", pr_l)\n",
    "print(\"Recall vallidation:    \", rec_l)\n",
    "print(\"Best F1: \", best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rndm_forest = RandomForestClassifier(max_depth=3)\n",
    "kf = KFold(n_splits=5)\n",
    "best_f1 = -1\n",
    "f1_l = []\n",
    "pr_l = []\n",
    "rec_l = []\n",
    "\n",
    "for train_ind, test_ind in kf.split(train_X,train_y):\n",
    "    clf_rndm_forest.fit(train_X[train_ind], train_y[train_ind])\n",
    "    predictions = clf_rndm_forest.predict(train_X[test_ind])\n",
    "    \n",
    "    f1 = f1_score(train_y[test_ind],predictions, average='weighted')\n",
    "    f1_l.append(f1)\n",
    "    pr = precision_score(train_y[test_ind],predictions, average='weighted')\n",
    "    pr_l.append(pr)\n",
    "    rec = recall_score(train_y[test_ind],predictions, average='weighted')\n",
    "    rec_l.append(rec)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = max(best_f1, f1)\n",
    "        best_X_train = train_X[train_ind]\n",
    "        best_y_train = train_y[train_ind]\n",
    "        best_X_test = train_X[test_ind]\n",
    "        best_y_test = train_y[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 vallidation:         [0.4688644688644689, 0.4688644688644689, 0.6688963210702342, 0.4688644688644689, 0.5333333333333333]\n",
      "Precision vallidation:  [0.37869822485207105, 0.37869822485207105, 0.591715976331361, 0.37869822485207105, 0.4444444444444444]\n",
      "Recall vallidation:     [0.6153846153846154, 0.6153846153846154, 0.7692307692307693, 0.6153846153846154, 0.6666666666666666]\n",
      "Best F1:  0.6688963210702342\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 vallidation:        \", f1_l)\n",
    "print(\"Precision vallidation: \", pr_l)\n",
    "print(\"Recall vallidation:    \", rec_l)\n",
    "print(\"Best F1: \", best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [2, 3, 4, 5]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='f1_weighted',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'max_depth':[2,3,4,5]}\n",
    "clf = GridSearchCV(RandomForestClassifier(), parameters, scoring = 'f1_weighted', cv=5)\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01097088, 0.00850983, 0.00937343, 0.00937281]),\n",
       " 'std_fit_time': array([0.00274967, 0.00501254, 0.00765337, 0.00765286]),\n",
       " 'mean_score_time': array([0.00159497, 0.00077329, 0.00312381, 0.00312409]),\n",
       " 'std_score_time': array([0.00048819, 0.00070784, 0.00624762, 0.00624819]),\n",
       " 'param_max_depth': masked_array(data=[2, 3, 4, 5],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 2},\n",
       "  {'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5}],\n",
       " 'split0_test_score': array([0.50310559, 0.50310559, 0.50310559, 0.50310559]),\n",
       " 'split1_test_score': array([0.62121212, 0.50310559, 0.63311688, 0.46753247]),\n",
       " 'split2_test_score': array([0.46886447, 0.46886447, 0.49230769, 0.60769231]),\n",
       " 'split3_test_score': array([0.53333333, 0.53333333, 0.53333333, 0.53333333]),\n",
       " 'split4_test_score': array([0.61244019, 0.61244019, 0.61244019, 0.61244019]),\n",
       " 'mean_test_score': array([0.54644575, 0.52060995, 0.55381182, 0.54102773]),\n",
       " 'std_test_score': array([0.06030827, 0.0464508 , 0.05810339, 0.0573281 ]),\n",
       " 'rank_test_score': array([2, 4, 1, 3]),\n",
       " 'split0_train_score': array([0.60873016, 0.74304762, 0.77065789, 0.79609412]),\n",
       " 'split1_train_score': array([0.64116667, 0.71274725, 0.67908471, 0.70615385]),\n",
       " 'split2_train_score': array([0.57329599, 0.65282398, 0.77466063, 0.79981725]),\n",
       " 'split3_train_score': array([0.6671317 , 0.6671317 , 0.69476587, 0.78300049]),\n",
       " 'split4_train_score': array([0.54025889, 0.57789757, 0.67591874, 0.78945331]),\n",
       " 'mean_train_score': array([0.60611668, 0.67072963, 0.71901757, 0.7749038 ]),\n",
       " 'std_train_score': array([0.04554692, 0.05647428, 0.04427903, 0.03485258])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 4}\n",
      "0.5538118239026419\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8018648018648018"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tree_best = RandomForestClassifier(max_depth=4, random_state=42)\n",
    "clf_tree_best.fit(best_X_train, best_y_train)\n",
    "predictions = clf_tree_best.predict(best_X_test)\n",
    "f1_score(best_y_test,predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_boost = GradientBoostingClassifier()\n",
    "kf = KFold(n_splits=5)\n",
    "best_f1 = -1\n",
    "f1_l = []\n",
    "pr_l = []\n",
    "rec_l = []\n",
    "\n",
    "for train_ind, test_ind in kf.split(train_X,train_y):\n",
    "    clf_boost.fit(train_X[train_ind], train_y[train_ind])\n",
    "    predictions = clf_boost.predict(train_X[test_ind])\n",
    "    \n",
    "    f1 = f1_score(train_y[test_ind],predictions, average='weighted')\n",
    "    f1_l.append(f1)\n",
    "    pr = precision_score(train_y[test_ind],predictions, average='weighted')\n",
    "    pr_l.append(pr)\n",
    "    rec = recall_score(train_y[test_ind],predictions, average='weighted')\n",
    "    rec_l.append(rec)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = max(best_f1, f1)\n",
    "        best_X_train = train_X[train_ind]\n",
    "        best_y_train = train_y[train_ind]\n",
    "        best_X_test = train_X[test_ind]\n",
    "        best_y_test = train_y[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 vallidation:         [0.6720647773279352, 0.5538461538461539, 0.6308985132514544, 0.4688644688644689, 0.49122807017543857]\n",
      "Precision vallidation:  [0.6013986013986015, 0.5934065934065934, 0.7208791208791209, 0.37869822485207105, 0.42424242424242425]\n",
      "Recall vallidation:     [0.7692307692307693, 0.5384615384615384, 0.6153846153846154, 0.6153846153846154, 0.5833333333333334]\n",
      "Best F1:  0.6720647773279352\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 vallidation:        \", f1_l)\n",
    "print(\"Precision vallidation: \", pr_l)\n",
    "print(\"Recall vallidation:    \", rec_l)\n",
    "print(\"Best F1: \", best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.2], 'n_estimators': [100, 150, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters={'learning_rate':[0.01, 0.05, 0.1, 0.2],'n_estimators':[100, 150, 200]}\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters, scoring = 'f1_weighted', cv=5)\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.2, 'n_estimators': 150}\n",
      "0.6228738205808639\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_boost_best = GradientBoostingClassifier(learning_rate=0.2,n_estimators=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6720647773279352"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_boost_best.fit(best_X_train, best_y_train)\n",
    "predictions = clf_boost_best.predict(best_X_test)\n",
    "f1_score(best_y_test,predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(columns=['doc_id', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['doc_id'] = test_df['doc_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,val in enumerate(submission_df['doc_id']):\n",
    "    if val not in test_y_id:\n",
    "        submission_df['prediction'][ind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_predictions = clf_tree_best.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_to_pred = {val:sub_predictions[ind] for ind,val in enumerate(test_y_id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,val in enumerate(submission_df['prediction']):\n",
    "    if val is np.NaN:\n",
    "        submission_df['prediction'][ind] = dict_id_to_pred[submission_df['doc_id'][ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    " submission_df.to_csv('Desktop/submission.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>109</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>113</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>134</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id prediction\n",
       "0        0          2\n",
       "1        2          2\n",
       "2        7          1\n",
       "3       15          1\n",
       "4       16          2\n",
       "5       24          2\n",
       "6       31          2\n",
       "7       32          2\n",
       "8       36          1\n",
       "9       38          2\n",
       "10      39          1\n",
       "11      43          2\n",
       "12      46          1\n",
       "13      47          2\n",
       "14      50          2\n",
       "15      51          2\n",
       "16      53          1\n",
       "17      56          2\n",
       "18      58          1\n",
       "19      59          1\n",
       "20      60          1\n",
       "21      62          1\n",
       "22      65          1\n",
       "23      66          2\n",
       "24      68          2\n",
       "25      69          2\n",
       "26      70          2\n",
       "27      71          1\n",
       "28      74          2\n",
       "29      78          2\n",
       "30      82          1\n",
       "31      84          2\n",
       "32      87          1\n",
       "33      91          1\n",
       "34      99          2\n",
       "35     103          2\n",
       "36     104          2\n",
       "37     109          2\n",
       "38     113          2\n",
       "39     116          1\n",
       "40     123          2\n",
       "41     124          1\n",
       "42     127          1\n",
       "43     134          2\n",
       "44     135          1\n",
       "45     142          1\n",
       "46     143          1\n",
       "47     147          1"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
